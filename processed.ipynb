{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを加工するノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 秋野編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "scores = df[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltkデータ読み込み\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各テキストのベクトルを計算する関数\n",
    "def text_to_vector(text, model):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(train_df):\n",
    "    # テキストとスコアを取得\n",
    "    texts = train_df[\"full_text\"]\n",
    "    \n",
    "\n",
    "    # テキストをトークン化\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # Word2Vecモデルの訓練\n",
    "    word2vec_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # 各テキストのベクトルを計算\n",
    "    text_vectors = np.array([text_to_vector(text, word2vec_model) for text in tokenized_texts])\n",
    "\n",
    "    return text_vectors, word2vec_model\n",
    "\n",
    "# DataFrameに含まれたテキストデータから、トークン化されたtext_vectorを取得\n",
    "text_vectors, word2vec_model = get_text_vectors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectors_df = pd.DataFrame(text_vectors)\n",
    "text_vectors_df['score'] = df[['score']].copy()\n",
    "# text_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors_with_model(train_df, word2vec_model):\n",
    "    # テキストとスコアを取得\n",
    "    texts = train_df[\"full_text\"]\n",
    "    \n",
    "    # テキストをトークン化\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # 各テキストのベクトルを計算\n",
    "    text_vectors = np.array([text_to_vector(text, word2vec_model) for text in tokenized_texts])\n",
    "\n",
    "    return text_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_randomForest(text_vectors_df, n_estimators, random_state=42):\n",
    "    feature_columns = [i for i in text_vectors_df.columns if i != \"score\"]\n",
    "    train_df = text_vectors_df[feature_columns]\n",
    "    target = text_vectors_df[[\"score\"]]\n",
    "\n",
    "    # データを訓練セットをテストセットに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.2, random_state=42)   \n",
    "\n",
    "    # ランダムフォレスト分類器を訓練\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    kappa_quadratic = cohen_kappa_score(y_test, y_pred, weights=\"quadratic\")\n",
    "    print(\"Weighted Kappa 二乗重み付け：\", kappa_quadratic)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_by_randomForest(text_vectors_df, n_estimators=100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測とSubmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの読み出し\n",
    "test_df = pd.read_csv(dataPath + \"/test.csv\")\n",
    "\n",
    "# DataFrameに含まれたテキストデータから、トークン化されたtext_vectorを取得\n",
    "test_text_vectors = get_text_vectors_with_model(test_df, word2vec_model)\n",
    "\n",
    "# text_vectorを使って、予測の実行\n",
    "test_pred = model.predict(test_text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df[[\"essay_id\"]].copy()\n",
    "submission_df['score'] = test_pred\n",
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import resample\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの数を確認\n",
    "df.groupby('score').apply(lambda x:x['score'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコア6の数に合わせても、156 x 6 = 936個のデータが取れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリングしたい総データ数\n",
    "total_sample_size = 2000\n",
    "\n",
    "# 各スコアからサンプリングするデータ数を計算\n",
    "unique_scores = df['score'].unique()\n",
    "min_count = min(df['score'].value_counts())\n",
    "sample_per_score = total_sample_size // len(unique_scores)\n",
    "\n",
    "# 各スコアごとにデータをサンプリング\n",
    "sampled_data = []\n",
    "\n",
    "for score in unique_scores:\n",
    "    score_data = df[df['score'] == score]\n",
    "    if len(score_data) >= sample_per_score:\n",
    "        sampled = resample(score_data, n_samples=sample_per_score, random_state=42)\n",
    "    else:\n",
    "        sampled = score_data\n",
    "    sampled_data.append(sampled)\n",
    "\n",
    "# サンプルデータを結合\n",
    "final_sample = pd.concat(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ数確認\n",
    "final_sample.groupby('score').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データ定義\n",
    "data = final_sample.copy()\n",
    "\n",
    "data['score'] = data['score'] - 1  # スコアを0-5に変換\n",
    "\n",
    "# データセットの分割\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['full_text'], data['score'], test_size=0.2, random_state=42, stratify=data['score'])\n",
    "\n",
    "# トークナイザーの準備\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# カスタムデータセットの作成\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = EssayDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_len=512)\n",
    "val_dataset = EssayDataset(val_texts.tolist(), val_labels.tolist(), tokenizer, max_len=512)\n",
    "\n",
    "# モデルの準備\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "\n",
    "# 評価指標の定義\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# トレーニング引数の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainerの作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# トレーニングの実行\n",
    "trainer.train()\n",
    "\n",
    "# 評価の実行\n",
    "trainer.evaluate()\n",
    "\n",
    "# モデルの出力\n",
    "model.save_pretrained('./bert-base-uncased-model-trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数:前半100個  \n",
    "{'eval_loss': 1.7269268035888672,\n",
    " 'eval_accuracy': 0.25,\n",
    " 'eval_f1': 0.2375,\n",
    " 'eval_precision': 0.48680555555555555,\n",
    " 'eval_recall': 0.25,\n",
    " 'eval_runtime': 3.1969,\n",
    " 'eval_samples_per_second': 6.256,\n",
    " 'eval_steps_per_second': 0.938,\n",
    " 'epoch': 3.0}  \n",
    " データ数:前半1,000個 実行時間:23min  \n",
    " {'eval_loss': 1.074561357498169,\n",
    " 'eval_accuracy': 0.52,\n",
    " 'eval_f1': 0.48400593321739693,\n",
    " 'eval_precision': 0.4655379723734051,\n",
    " 'eval_recall': 0.52,\n",
    " 'eval_runtime': 30.6655,\n",
    " 'eval_samples_per_second': 6.522,\n",
    " 'eval_steps_per_second': 0.815,\n",
    " 'epoch': 3.0}  \n",
    " データ数:936個 各スコア156個ずつ 実行時間:21min  \n",
    " {'eval_loss': 1.1392934322357178,\n",
    " 'eval_accuracy': 0.5,\n",
    " 'eval_f1': 0.4120386813326839,\n",
    " 'eval_precision': 0.5260695493022192,\n",
    " 'eval_recall': 0.5,\n",
    " 'eval_runtime': 29.1307,\n",
    " 'eval_samples_per_second': 6.454,\n",
    " 'eval_steps_per_second': 0.824,\n",
    " 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割しておいたテストデータを用いて、重み付きKappaの計算をする\n",
    "# 評価モードに設定\n",
    "model.eval()\n",
    "\n",
    "# データをトークン化\n",
    "encoded_input = tokenizer(val_texts.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# モデルに入力を与えて推論を行う\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解データからラベルを抽出\n",
    "true_labels = val_labels.tolist()\n",
    "# 予測データを取得\n",
    "predicted_labels = predictions\n",
    "# 重み付きKappaを計算\n",
    "weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights='quadratic')\n",
    "print('重み付きKappa:', weighted_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle提出用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (kaggle提出用)テスト用データへの適用\n",
    "# テストデータの読み出し\n",
    "test_df = pd.read_csv(dataPath + \"/test.csv\")\n",
    "test_texts = test_df['full_text'].copy()\n",
    "\n",
    "# データをトークン化\n",
    "test_encoded_input = tokenizer(test_texts.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# モデルに入力を与えて推論を行う\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encoded_input)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df[[\"essay_id\"]].copy()\n",
    "submission_df['score'] = predictions\n",
    "submission_df['score'] = submission_df['score'] + 1 # 予測結果は0-5で出力されるので +1して元データの1-6に合わせる\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済BERTモデルを使った推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_bert(bert_model, input_token) -> list():\n",
    "    \"\"\"\n",
    "    既存BERTモデルを使った推論\n",
    "\n",
    "    Args:\n",
    "        bert_model: 学習済BERTモデル\n",
    "        input_token: torkenizeされた入力データ\n",
    "\n",
    "    Returns:\n",
    "        predictions: 推論結果のリスト\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**input_token)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, axis=1).tolist()\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済モデルの読み出し\n",
    "model_path = '/data/bert-base-uncased-model-trained'\n",
    "if os.path.isdir(model_path):\n",
    "    model_trained = BertForSequenceClassification.from_pretrained(model_path, num_labels=6)\n",
    "    model_trained.eval()\n",
    "else:\n",
    "    print('モデルフォルダがない')\n",
    "\n",
    "# tokenizerの読み出し\n",
    "tokenizer_path = '/data/bert-base-uncased-tokenizer'\n",
    "if os.path.isdir(tokenizer_path):\n",
    "    tokenizer  = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "else:\n",
    "    print('トークナイザーのフォルダがない')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "eval_data = df.copy()\n",
    "eval_data = df[~df['essay_id'].isin(final_sample['essay_id'])]\n",
    "eval_data = eval_data.sample(100, random_state=42)\n",
    "eval_texts = eval_data['full_text'].tolist()\n",
    "input_token = tokenizer(eval_texts, padding=True, truncation=True, return_tensors='pt', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_with_bert(model_trained, input_token)\n",
    "\n",
    "# Kappaの計算\n",
    "# 正解データからラベルを抽出\n",
    "true_labels = eval_data['score'].tolist()\n",
    "# 予測データを取得\n",
    "predicted_labels = predictions\n",
    "# 重み付きKappaを計算\n",
    "weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights='quadratic')\n",
    "print('重み付きKappa:', weighted_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済BERTを使った推論2 kaggleのメモリ不足対策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- メモリ効率の高い推論ループを使用する  \n",
    "- 評価時バッチサイズを 8 -> 4に変更する  \n",
    "BERTモデルは上のコードから流用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムデータセットの作成\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "eval_data = df.copy()\n",
    "eval_data = df[~df['essay_id'].isin(final_sample['essay_id'])]\n",
    "eval_data = eval_data.sample(100, random_state=42)\n",
    "eval_dataset = EssayDataset(eval_data['full_text'].tolist(), tokenizer=tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 評価時のバッチサイズを指定\n",
    "eval_batch_size = 4\n",
    "\n",
    "# データローダーの作成\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size)\n",
    "\n",
    "# モデルを評価モードに設定\n",
    "model_trained.eval()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 推論結果を保存するリスト\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model_trained(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# 予測結果の確認\n",
    "print(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kappaの計算\n",
    "# 正解データからラベルを抽出\n",
    "true_labels = eval_data['score'].tolist()\n",
    "# 予測データを取得\n",
    "predicted_labels = all_predictions\n",
    "# 重み付きKappaを計算\n",
    "weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights='quadratic')\n",
    "print('重み付きKappa:', weighted_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERTの改良型で、計算効率向上ができるかもしれない  \n",
    "- 現状のBERTは学習自体に時間かかる & kaggle提出時の推論もかなり時間かかっているので、改善するか試してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリングしたい総データ数\n",
    "total_sample_size = 2000\n",
    "\n",
    "# 各スコアからサンプリングするデータ数を計算\n",
    "unique_scores = df['score'].unique()\n",
    "min_count = min(df['score'].value_counts())\n",
    "sample_per_score = total_sample_size // len(unique_scores)\n",
    "\n",
    "# 各スコアごとにデータをサンプリング\n",
    "sampled_data = []\n",
    "\n",
    "for score in unique_scores:\n",
    "    score_data = df[df['score'] == score]\n",
    "    if len(score_data) >= sample_per_score:\n",
    "        sampled = resample(score_data, n_samples=sample_per_score, random_state=42)\n",
    "    else:\n",
    "        sampled = score_data\n",
    "    sampled_data.append(sampled)\n",
    "\n",
    "# サンプルデータを結合\n",
    "final_sample = pd.concat(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データ定義\n",
    "data = final_sample.copy()\n",
    "data['score'] = data['score'] - 1  # スコアを0-5に変換\n",
    "\n",
    "# データセットの分割\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['full_text'], data['score'], test_size=0.2)\n",
    "\n",
    "# DeBERTa用トークナイザーの準備\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムデータセットの作成\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムデータセット化\n",
    "train_dataset = EssayDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_len=512)\n",
    "val_dataset = EssayDataset(val_texts.tolist(), val_labels.tolist(), tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# DeBERTaモデルの準備\n",
    "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標の定義\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# トレーニング引数の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerの作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すぐカーネルクラッシュしてしまうので、DeBERTa学習はローカルで行い、学習済モデルを使用することにする\n",
    "\n",
    "# # トレーニングの実行\n",
    "# trainer.train()\n",
    "\n",
    "# # 評価の実行\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済DeBERTaを使った推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 02:30:44.800767: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 02:30:44.868506: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-24 02:30:45.166900: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-24 02:30:45.166944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-24 02:30:45.220081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-24 02:30:45.328832: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-24 02:30:45.330152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 02:30:46.201823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "\n",
    "# 学習用データ定義\n",
    "data = df.copy()\n",
    "data['score'] = data['score'] - 1  # スコアを0-5に変換\n",
    "\n",
    "# データセットの分割\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['full_text'], data['score'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルに保存したトークナイザーのロード\n",
    "tokenizer = DebertaTokenizer.from_pretrained('/data/deberta-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムデータセットの作成\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認用のテストデータ\n",
    "val_dataset = EssayDataset(val_texts.tolist(), tokenizer=tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済モデルのロード\n",
    "model = DebertaForSequenceClassification.from_pretrained('/data/deberta-model-trained', num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの作成\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaForSequenceClassification(\n",
       "  (deberta): DebertaModel(\n",
       "    (embeddings): DebertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
       "      (LayerNorm): DebertaLayerNorm()\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(1024, 768)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論の実行\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重み付きKappa: 0.7520735832970541\n"
     ]
    }
   ],
   "source": [
    "# Kappaの計算\n",
    "# 正解データからラベルを抽出\n",
    "true_labels = val_labels.tolist()\n",
    "# 予測データを取得\n",
    "predicted_labels = predictions\n",
    "# 重み付きKappaを計算\n",
    "weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights='quadratic')\n",
    "print('重み付きKappa:', weighted_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岡本編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df):\n",
    "    \"\"\"特徴量作成関数\n",
    "\n",
    "    特徴量の説明\n",
    "        text_len:テキストの長さ\n",
    "        space_count:空白の数\n",
    "        word_len_avg:一節の平均的な長さ\n",
    "        I-cnt:”私”という単語の出現頻度\n",
    "\n",
    "    Args:\n",
    "        df(pandas.DataFrame):加工したいデータフレーム\n",
    "    Return:\n",
    "        pandas.DataFrame:加工後のデータフレーム\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['text_len'] = df.full_text.str.len()\n",
    "    df['space_count'] = df.full_text.str.count(' ')\n",
    "    df['word_len_avg'] = (df.text_len - df.space_count) / (df.space_count + 1)\n",
    "    df['I-cnt'] = df.full_text.str.startswith('I') + df.full_text.str.count('. I ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = processing(train_df)\n",
    "processed_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test = processing(test_df)\n",
    "processed_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語特徴量作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の出現頻度確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_freq(df):\n",
    "    #インスタンス生成\n",
    "    vec_count = CountVectorizer()\n",
    "    vec_count.fit(df.full_text)\n",
    "    X = vec_count.transform(df.full_text)\n",
    "    #単語をカラム化してデータフレームに追加\n",
    "    word_df = pd.DataFrame(X.toarray())\n",
    "    word_df.columns = vec_count.get_feature_names_out()\n",
    "    #df = pd.concat([df, word_df], axis=1)\n",
    "    #単語の出現頻度データフレーム作成\n",
    "    #word_df = pd.DataFrame(word_df.sum(axis=0).sort_values(ascending=False).reset_index())\n",
    "    #word_df.columns = ['word', 'count']\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def split_data(df):\n",
    "    num = len(df) // 4\n",
    "    q2 = num * 2\n",
    "    q3 = num * 3\n",
    "    q1_df = df.iloc[:num,:]\n",
    "    q2_df = df.iloc[num:q2,:]\n",
    "    q3_df = df.iloc[q2:q3,:]\n",
    "    q4_df = df.iloc[q3:,:]\n",
    "    return q1_df,q2_df,q3_df,q4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df_train = check_freq(processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([processed_train,word_df_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df_test = check_freq(processed_test)\n",
    "test_df = pd.concat([processed_test,word_df_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('/data/add_word_train.csv',index=False)\n",
    "test_df.to_csv('/data/add_word_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワード削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/data/add_word_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/data/add_word_test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#英語のストップワードダウンロード\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "#ストップワードを集合として格納（後で集合同士の比較演算を行うため）\n",
    "stop_words_set = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopword(df):\n",
    "    \n",
    "    #単語データフレーム内にどれだけストップワードが含まれるのか確認\n",
    "    columns_set = set(df.columns)\n",
    "\n",
    "    #共通単語抽出\n",
    "    and_set = columns_set & stop_words_set\n",
    "\n",
    "    #単語データフレームに存在しなかったストップワード数抽出\n",
    "    before = len(df.columns)\n",
    "    tmp_df = df.drop(columns=list(and_set))\n",
    "    after = len(tmp_df.columns)\n",
    "\n",
    "    #ストップワードの除去、除去前後で矛盾がないか確認\n",
    "    print(f'処理前カラム数：{before} 処理後カラム数：{after} 差：{before-after}')\n",
    "    if before - after == len(and_set):\n",
    "        print('処理に問題はありません')\n",
    "    else:\n",
    "        print('処理に矛盾が発生しています')\n",
    "\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_stopword(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = drop_stopword(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.sum().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_sum = tmp_df.sum(numeric_only=True).to_frame()\n",
    "tmp_sum.columns = ['count']\n",
    "tmp_sum = tmp_sum.sort_values('count',ascending=False).round(1)\n",
    "tmp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=((15,8)))\n",
    "x = np.arange(1,11)\n",
    "y = []\n",
    "l = [len(train_df.columns)] * 10\n",
    "for i in range(1, 11):\n",
    "    y.append(len(tmp_sum.query('count <= @i')))\n",
    "\n",
    "ax[0].plot(x, y, marker='.', markersize=10)\n",
    "ax[0].set_title('単語の出現頻度の閾値 VS 落とされる特徴量')\n",
    "ax[0].set_xlabel('単語の出現頻度の閾値')\n",
    "ax[0].set_ylabel('落とされる特徴量')\n",
    "for i, txt in enumerate(y):\n",
    "    ax[0].text(x[i], y[i], txt)\n",
    "\n",
    "y = np.array(l) - np.array(y)\n",
    "ax[1].plot(x, y, marker='.', markersize=10)\n",
    "ax[1].set_title('単語の出現頻度の閾値 VS 残る特徴量数')\n",
    "ax[1].set_xlabel('単語の出現頻度の閾値')\n",
    "ax[1].set_ylabel('残る特徴量数')\n",
    "for i, txt in enumerate(y):\n",
    "    ax[1].text(x[i], y[i], txt)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出現頻度が３以下の特徴量を削除することでレコード数よりも特徴量数を抑えられる\n",
    "- ただ、テストデータも同様のデータ構造であることが前提となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理後データ出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/processed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
