{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを加工するノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 秋野編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "scores = df[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltkデータ読み込み\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各テキストのベクトルを計算する関数\n",
    "def text_to_vector(text, model):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(train_df):\n",
    "    # テキストとスコアを取得\n",
    "    texts = train_df[\"full_text\"]\n",
    "    \n",
    "\n",
    "    # テキストをトークン化\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # Word2Vecモデルの訓練\n",
    "    word2vec_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # 各テキストのベクトルを計算\n",
    "    text_vectors = np.array([text_to_vector(text, word2vec_model) for text in tokenized_texts])\n",
    "\n",
    "    return text_vectors, word2vec_model\n",
    "\n",
    "# DataFrameに含まれたテキストデータから、トークン化されたtext_vectorを取得\n",
    "text_vectors, word2vec_model = get_text_vectors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectors_df = pd.DataFrame(text_vectors)\n",
    "text_vectors_df['score'] = df[['score']].copy()\n",
    "# text_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors_with_model(train_df, word2vec_model):\n",
    "    # テキストとスコアを取得\n",
    "    texts = train_df[\"full_text\"]\n",
    "    \n",
    "    # テキストをトークン化\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # 各テキストのベクトルを計算\n",
    "    text_vectors = np.array([text_to_vector(text, word2vec_model) for text in tokenized_texts])\n",
    "\n",
    "    return text_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_randomForest(text_vectors_df, n_estimators, random_state=42):\n",
    "    feature_columns = [i for i in text_vectors_df.columns if i != \"score\"]\n",
    "    train_df = text_vectors_df[feature_columns]\n",
    "    target = text_vectors_df[[\"score\"]]\n",
    "\n",
    "    # データを訓練セットをテストセットに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.2, random_state=42)   \n",
    "\n",
    "    # ランダムフォレスト分類器を訓練\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    kappa_quadratic = cohen_kappa_score(y_test, y_pred, weights=\"quadratic\")\n",
    "    print(\"Weighted Kappa 二乗重み付け：\", kappa_quadratic)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_by_randomForest(text_vectors_df, n_estimators=100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測とSubmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの読み出し\n",
    "test_df = pd.read_csv(dataPath + \"/test.csv\")\n",
    "\n",
    "# DataFrameに含まれたテキストデータから、トークン化されたtext_vectorを取得\n",
    "test_text_vectors = get_text_vectors_with_model(test_df, word2vec_model)\n",
    "\n",
    "# text_vectorを使って、予測の実行\n",
    "test_pred = model.predict(test_text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df[[\"essay_id\"]].copy()\n",
    "submission_df['score'] = test_pred\n",
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの数を確認\n",
    "df.groupby('score').apply(lambda x:x['score'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコア6の数に合わせても、156 x 6 = 936個のデータが取れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコア毎に同数のデータを取得する\n",
    "min_samples = df.groupby('score').size().min()\n",
    "balanced_data = df.groupby('score').apply(lambda x:x.sample(n=min_samples)).reset_index(drop=True)\n",
    "balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatGPTサンプルコード\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "\n",
    "# データセットの読み込み\n",
    "# data = df.head(1000)\n",
    "\n",
    "# スコア毎に同数のデータを取得する\n",
    "min_samples = df.groupby('score').size().min()\n",
    "data = df.groupby('score').apply(lambda x:x.sample(n=min_samples)).reset_index(drop=True)\n",
    "\n",
    "data['score'] = data['score'] - 1  # スコアを0-5に変換\n",
    "\n",
    "# データセットの分割\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['full_text'], data['score'], test_size=0.2, random_state=42, stratify=data['score'])\n",
    "\n",
    "# トークナイザーの準備\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# カスタムデータセットの作成\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = EssayDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, max_len=512)\n",
    "val_dataset = EssayDataset(val_texts.tolist(), val_labels.tolist(), tokenizer, max_len=512)\n",
    "\n",
    "# モデルの準備\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "\n",
    "# 評価指標の定義\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# トレーニング引数の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainerの作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# トレーニングの実行\n",
    "trainer.train()\n",
    "\n",
    "# 評価の実行\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数:前半100個  \n",
    "{'eval_loss': 1.7269268035888672,\n",
    " 'eval_accuracy': 0.25,\n",
    " 'eval_f1': 0.2375,\n",
    " 'eval_precision': 0.48680555555555555,\n",
    " 'eval_recall': 0.25,\n",
    " 'eval_runtime': 3.1969,\n",
    " 'eval_samples_per_second': 6.256,\n",
    " 'eval_steps_per_second': 0.938,\n",
    " 'epoch': 3.0}  \n",
    " データ数:前半1,000個 実行時間:23min  \n",
    " {'eval_loss': 1.074561357498169,\n",
    " 'eval_accuracy': 0.52,\n",
    " 'eval_f1': 0.48400593321739693,\n",
    " 'eval_precision': 0.4655379723734051,\n",
    " 'eval_recall': 0.52,\n",
    " 'eval_runtime': 30.6655,\n",
    " 'eval_samples_per_second': 6.522,\n",
    " 'eval_steps_per_second': 0.815,\n",
    " 'epoch': 3.0}  \n",
    " データ数:936個 各スコア156個ずつ 実行時間:21min  \n",
    " {'eval_loss': 1.1392934322357178,\n",
    " 'eval_accuracy': 0.5,\n",
    " 'eval_f1': 0.4120386813326839,\n",
    " 'eval_precision': 0.5260695493022192,\n",
    " 'eval_recall': 0.5,\n",
    " 'eval_runtime': 29.1307,\n",
    " 'eval_samples_per_second': 6.454,\n",
    " 'eval_steps_per_second': 0.824,\n",
    " 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割しておいたテストデータを用いて、重み付きKappaの計算をする\n",
    "# 評価モードに設定\n",
    "model.eval()\n",
    "\n",
    "# データをトークン化\n",
    "encoded_input = tokenizer(val_texts.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# モデルに入力を与えて推論を行う\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解データからラベルを抽出\n",
    "true_labels = val_labels.tolist()\n",
    "# 予測データを取得\n",
    "predicted_labels = predictions\n",
    "# 重み付きKappaを計算\n",
    "weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (kaggle提出用)テスト用データへの適用\n",
    "# テストデータの読み出し\n",
    "test_df = pd.read_csv(dataPath + \"/test.csv\")\n",
    "test_texts = test_df['full_text'].copy()\n",
    "\n",
    "# データをトークン化\n",
    "test_encoded_input = tokenizer(test_texts.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# モデルに入力を与えて推論を行う\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encoded_input)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df[[\"essay_id\"]].copy()\n",
    "submission_df['score'] = predictions\n",
    "submission_df['score'] = submission_df['score'] + 1 # 予測結果は0-5で出力されるので +1して元データの1-6に合わせる\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岡本編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df):\n",
    "    \"\"\"特徴量作成関数\n",
    "\n",
    "    特徴量の説明\n",
    "        text_len:テキストの長さ\n",
    "        space_count:空白の数\n",
    "        word_len_avg:一節の平均的な長さ\n",
    "        I-cnt:”私”という単語の出現頻度\n",
    "\n",
    "    Args:\n",
    "        df(pandas.DataFrame):加工したいデータフレーム\n",
    "    Return:\n",
    "        pandas.DataFrame:加工後のデータフレーム\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['text_len'] = df.full_text.str.len()\n",
    "    df['space_count'] = df.full_text.str.count(' ')\n",
    "    df['word_len_avg'] = (df.text_len - df.space_count) / (df.space_count + 1)\n",
    "    df['I-cnt'] = df.full_text.str.startswith('I') + df.full_text.str.count('. I ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = processing(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = processing(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text):\n",
    "    try:\n",
    "        translated = translator.translate(text,src='en',dest='ja').text\n",
    "        return translated\n",
    "    except Exception as e:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_ja'] = train_df.full_text.progress_apply(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#翻訳後データフレームをcsv出力\n",
    "train_df.to_csv('./data/trandlated_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理後データ出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/processed_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/processed_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
