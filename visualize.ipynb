{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ可視化を行うノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岡本編集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "sns.countplot(x='score',data=train_df,ax=ax)\n",
    "ax.set_title('スコアごとのデータ件数')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_len'] = train_df.full_text.str.len()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章が長いほどスコアが高いのではないか\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(train_df.score,train_df.text_len,alpha=0.3)\n",
    "ax.set_xlabel('スコア')\n",
    "ax.set_ylabel('文字列の長さ')\n",
    "ax.set_title('スコア VS 文字列の長さ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['space_count'] = train_df.full_text.str.count(' ')\n",
    "train_df['word_len_avg'] = (train_df.text_len - train_df.space_count) / (train_df.space_count + 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一節が長めだとスコアが高いのではないか？\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_df.score,train_df.word_len_avg,alpha=0.3)\n",
    "ax.set_title('スコア VS 平均的な一節の文字列長さ')\n",
    "ax.set_xlabel('スコア')\n",
    "ax.set_ylabel('平均文字列長')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#「I」で始まる小論文は自分が足りでスコアが低いのではないか\n",
    "train_df['I-cnt'] = train_df.full_text.str.startswith('I') + train_df.full_text.str.count('. I ')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(train_df.score,train_df['I-cnt'],alpha=0.3)\n",
    "ax.set_title('スコア VS I-cnt')\n",
    "ax.set_xlabel('スコア')\n",
    "ax.set_ylabel('I-cnt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in train_df.select_dtypes(include=int).columns:\n",
    "    sns.histplot(x=feature,data=train_df,kde=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['log_text_len'] = np.log(train_df.text_len)\n",
    "train_df['log_space_count'] = np.log(train_df.space_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='log_text_len', data=train_df,kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='log_space_count',data=train_df,kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 秋野編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "\n",
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "scores = df[\"score\"]\n",
    "texts = df[\"full_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全体の単語の使用回数を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストをトークン化\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ確認\n",
    "# pd.DataFrame(tokenized_texts).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の使用回数を辞書として抽出する\n",
    "word_dic = {}\n",
    "for text in tokenized_texts:\n",
    "    for word in text:\n",
    "        if word in word_dic:\n",
    "            word_dic[word] += 1\n",
    "        else:\n",
    "            word_dic[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の使用回数をDataFrame型に変換する\n",
    "# 回数の多い順でソートしておく\n",
    "sorted_data = sorted(word_dic.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_word_dic = dict(sorted_data)\n",
    "\n",
    "# 辞書をDataFrameに変換\n",
    "word_df = pd.DataFrame(list(sorted_word_dic.items()), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ確認\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の使用回数をグラフ表示する\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.bar(x=word_df['word'][:30], height=word_df['count'][:30])\n",
    "ax.set_xlabel('単語')\n",
    "ax.set_ylabel('使用回数')\n",
    "ax.set_title('全テキストの単語別使用回数 上位のみ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン数の分布の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token化したリストをDataFrameに変換\n",
    "token_df = pd.DataFrame(tokenized_texts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = []\n",
    "for i in range(len(token_df.columns)):\n",
    "    token_count.append(len([word for word in token_df[i] if word is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(token_count, bins=20)\n",
    "ax.set_xlabel('トークン数')\n",
    "ax.set_ylabel('データ個数')\n",
    "ax.set_title('トークン数の分布')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "かなりトークン数多め  \n",
    "Stopword削除でどこまでへるか確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordを除去した時のトークン数分布\n",
    "# NLTKのデータをダウンロード\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ストップワードのリストを取得\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# テキストからストップワードを削除する関数\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordを削除しつつ、token数の分布を表示する\n",
    "df_no_stopwords = df.copy()\n",
    "df_no_stopwords['full_text'] = df_no_stopwords['full_text'].apply(remove_stopwords)\n",
    "texts_no_stopwords = df_no_stopwords['full_text']\n",
    "# テキストをトークン化\n",
    "tokenized_texts_no_stopwords = [word_tokenize(text.lower()) for text in texts_no_stopwords]\n",
    "\n",
    "# token化したリストをDataFrameに変換\n",
    "token_df = pd.DataFrame(tokenized_texts_no_stopwords).T\n",
    "\n",
    "token_count = []\n",
    "for i in range(len(token_df.columns)):\n",
    "    token_count.append(len([word for word in token_df[i] if word is not None]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(token_count, bins=20)\n",
    "ax.set_xlabel('トークン数')\n",
    "ax.set_ylabel('データ個数')\n",
    "ax.set_title('トークン数の分布')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopword削除により、ほぼ500以下のトークンになっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スコアによって、使用単語の傾向が変わるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用単語回数と、スコアの相関関係を見る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なNLTKデータのダウンロード\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stopword(意味のない単語)を設定\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 小文字に変換\n",
    "    text = text.lower()\n",
    "    # 単語にトークン化\n",
    "    words = word_tokenize(text)\n",
    "    # ストップワードの除去\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "df = df.head(8000).copy() # データ少な目で実験。全データにするとなぜかクラッシュする\n",
    "scores = df[\"score\"]\n",
    "texts = df[\"full_text\"]\n",
    "\n",
    "df['processed_text'] = df['full_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizerを使って単語の出現頻度をベクトル化\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# ベクトルをデータフレームに変換\n",
    "words_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# スコアを追加\n",
    "words_df['score'] = df['score']\n",
    "\n",
    "# データ確認\n",
    "# words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相関計算のために単語列のみを抽出\n",
    "words_only_df = words_df.drop(columns=['score'])\n",
    "\n",
    "# 相関を計算し、結果をDataFrameに格納\n",
    "correlations = {}\n",
    "for word in words_only_df.columns:\n",
    "    correlation, _ = pearsonr(words_only_df[word], words_df['score'])\n",
    "    correlations[word] = correlation\n",
    "\n",
    "# 相関(絶対値)の高い順にソート\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# 相関上位の単語とその相関値\n",
    "top_words = pd.DataFrame(sorted_correlations, columns=['word', 'correlation'])\n",
    "\n",
    "# データ確認\n",
    "top_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正の相関の強い単語をグラフ表示\n",
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "ax.bar(x=top_words['word'][:50], height=top_words['correlation'][:50])\n",
    "ax.set_xlabel('単語')\n",
    "ax.set_ylabel('相関係数')\n",
    "ax.set_title('単語の出現回数とスコアの相関関係')\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負の相関の強い単語をグラフ表示\n",
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "ax.bar(x=top_words['word'][-50:], height=top_words['correlation'][-50:])\n",
    "ax.set_xlabel('単語')\n",
    "ax.set_ylabel('相関係数')\n",
    "ax.set_title('単語の出現回数とスコアの相関関係')\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正の相関にしろ、負の相関にしろ、大した相関関係はなさそう  \n",
    "正の相関の上位は少し相関があるが、あまりキーワード的な言葉には見えない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 3,
           "length": 1,
           "op": "removerange"
          },
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "-"
          },
          {
           "key": 6,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "key": "version",
       "op": "remove"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
